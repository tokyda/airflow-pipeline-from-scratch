# Airflow Pipeline From Scratch

An end-to-end learning project where I rebuilt a production-style analytics pipeline from scratch using Apache Airflow.  
The pipeline ingests data from (1) a CSV file drop and (2) a Postgres source table, stages raw data, and prepares downstream artifacts for analytics and dashboards.

## What this project demonstrates
- Airflow DAG design: tasks, dependencies, and orchestration
- Multi-source ingestion: file-based (CSV) + database-based (Postgres SQL)
- Data staging patterns: source → raw (standardised location for downstream jobs)
- Reproducible local setup using Docker Compose

## Architecture (high level)
1. **Source**
   - IMDb movie reviews (CSV file drop)
   - Online Retail transactions (UCI dataset loaded into Postgres)
2. **Orchestration**
   - Airflow schedules and runs each task in order
3. **Staging**
   - Raw outputs are written to `data/raw/`

## Repo structure
- `dags/` — Airflow DAG definitions
- `scripts/` — helper scripts (e.g., dataset downloads)
- `data/source/` — source datasets (not committed to Git)
- `data/raw/` — staged raw outputs (generated by pipeline, not committed)
- `docker-compose.yml` — local Airflow + Postgres setup

## Getting started (local)
### Prerequisites
- Docker Desktop (running)
- Git

### Start Airflow
```bash
docker compose up -d

Open Airflow UI:
- http://localhost:8080
- Username: airflow
- Password: airflow

## Data setup (do not commit large files)

This repo does not store raw datasets in Git because they are large.  
Instead, download them locally into `data/source/`.

### Dataset 1: IMDb Movie Reviews

Download via the provided script:
```bash
pip install huggingface_hub
python scripts/download_imdb.py

### Dataset 2: Online Retail (UCI)

1. Download the dataset from the UCI Machine Learning Repository ("Online Retail Dataset").
2. Convert to CSV if needed.
3. Save as:
- `data/source/OnlineRetail.csv`

## Phase checkpoints (progress)

- [ ] Phase 1: Minimal Airflow DAG runs end-to-end
- [ ] Phase 2 Step 0–2: Set up source/raw folders and obtain datasets
- [ ] Phase 2 Step 3: Load OnlineRetail into Postgres and extract both sources into `data/raw/`
- [ ] Phase 3: Processing + metrics
- [ ] Phase 4: Dashboard generation

## License / attribution

This project is inspired by the “beginner_de_project” tutorial by Joseph Machado.  
I re-implemented the pipeline to learn end-to-end data engineering workflows and adapted components for my own structure and documentation.

